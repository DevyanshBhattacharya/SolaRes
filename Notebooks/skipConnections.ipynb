{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6245bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cfe46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentations\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "def radial_vignette(img, strength=0.2):\n",
    "    \"\"\"\n",
    "    Apply radial vignette (simulate limb-darkening).\n",
    "    strength: 0.0 (none) to 0.5 (strong)\n",
    "    \"\"\"\n",
    "    h, w = img.shape\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    cy, cx = h / 2, w / 2\n",
    "    r = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "    r = r / r.max()  # normalize radius [0,1]\n",
    "    \n",
    "    mask = 1 - strength * (r**2)  \n",
    "    vignette = img.astype(np.float32) * mask\n",
    "    vignette = np.clip(vignette, 0, 255)\n",
    "    return vignette.astype(np.uint8)\n",
    "\n",
    "def add_poisson_noise(img, scale_low=5.0, scale_high=100.0):\n",
    "    \"\"\"\n",
    "    Add Poisson noise to simulate photon noise.\n",
    "    \"\"\"\n",
    "    if img.dtype != np.float32:\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "    scale = random.uniform(scale_low, scale_high)\n",
    "    noisy = np.random.poisson(img * scale) / float(scale)\n",
    "    noisy = np.clip(noisy, 0.0, 1.0)\n",
    "    return (noisy * 255).astype(np.uint8)\n",
    "\n",
    "def random_rotation(img, angle_range=20):\n",
    "    \"\"\"\n",
    "    Rotate image by random angle within ±angle_range.\n",
    "    \"\"\"\n",
    "    h, w = img.shape\n",
    "    angle = random.uniform(-angle_range, angle_range)\n",
    "    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "    rotated = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    return rotated\n",
    "\n",
    "\n",
    "def random_crop_resize(img, crop_scale=0.8, out_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Random crop followed by resize.\n",
    "    crop_scale: fraction of original image kept (0.5–1.0).\n",
    "    \"\"\"\n",
    "    h, w = img.shape\n",
    "    ch, cw = int(h * crop_scale), int(w * crop_scale)\n",
    "    y = random.randint(0, h - ch)\n",
    "    x = random.randint(0, w - cw)\n",
    "    crop = img[y:y+ch, x:x+cw]\n",
    "    resized = cv2.resize(crop, out_size, interpolation=cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma_range=(0.5, 2.0)):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur with random sigma.\n",
    "    \"\"\"\n",
    "    sigma = random.uniform(*sigma_range)\n",
    "    ksize = int(2 * round(3*sigma) + 1) \n",
    "    blurred = cv2.GaussianBlur(img, (ksize, ksize), sigmaX=sigma)\n",
    "    return blurred\n",
    "\n",
    "\n",
    "def brightness_contrast_jitter(img, brightness=0.2, contrast=0.3):\n",
    "    \"\"\"\n",
    "    Random brightness/contrast adjustment.\n",
    "    brightness: fraction (e.g. 0.2 → ±20%)\n",
    "    contrast: fraction (e.g. 0.3 → ±30%)\n",
    "    \"\"\"\n",
    "    b = random.uniform(-brightness, brightness) * 255\n",
    "    c = 1.0 + random.uniform(-contrast, contrast)\n",
    "    jittered = img.astype(np.float32) * c + b\n",
    "    jittered = np.clip(jittered, 0, 255)\n",
    "    return jittered.astype(np.uint8)\n",
    "\n",
    "def random_noise(img, noise_level=0.10):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise.\n",
    "    noise_level: fraction of max pixel value (0.0-0.5)\n",
    "    \"\"\"\n",
    "    if img.dtype != np.float32:\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "    noise = np.random.normal(0, noise_level, img.shape)\n",
    "    noisy = img + noise\n",
    "    noisy = np.clip(noisy, 0.0, 1.0)\n",
    "    return (noisy * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a6ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 36832  100 36832    0     0  69592      0 --:--:-- --:--:-- --:--:-- 69889\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/models/network_swinir.py -o swinir_model.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd4108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "c:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from swinir_model import SwinIR\n",
    "model = SwinIR(\n",
    "    upscale=4,\n",
    "    in_chans=3,\n",
    "    img_size=64,\n",
    "    window_size=8,\n",
    "    img_range=1.0,\n",
    "    depths=[6,6,6,6],\n",
    "    embed_dim=60,\n",
    "    num_heads=[6,6,6,6],\n",
    "    mlp_ratio=2,\n",
    "    upsampler='pixelshuffle'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d12729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, augment=True, img_size=(256,256),max_images=None):\n",
    "        self.lr_files = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir)])\n",
    "        self.hr_files = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir)])\n",
    "        if max_images is not None:\n",
    "            self.lr_files = self.lr_files[:max_images]\n",
    "            self.hr_files = self.hr_files[:max_images]\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images\n",
    "        lr = cv2.imread(self.lr_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        hr = cv2.imread(self.hr_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize to same size\n",
    "        lr = cv2.resize(lr, self.img_size)\n",
    "        hr = cv2.resize(hr, self.img_size)\n",
    "\n",
    "        # Apply augmentations to LR only\n",
    "        if self.augment:\n",
    "            lr = radial_vignette(lr, strength=random.uniform(0.05,0.2))\n",
    "            lr = add_poisson_noise(lr)\n",
    "            lr = random_rotation(lr, angle_range=15)\n",
    "            lr = gaussian_blur(lr, sigma_range=(0.5,1.5))\n",
    "            lr = brightness_contrast_jitter(lr, brightness=0.15, contrast=0.2)\n",
    "            lr = random_noise(lr, noise_level=0.05)\n",
    "        \n",
    "        # Normalize and convert to tensor [C,H,W]\n",
    "        lr = torch.tensor(lr, dtype=torch.float32).unsqueeze(0)/255.0\n",
    "        hr = torch.tensor(hr, dtype=torch.float32).unsqueeze(0)/255.0\n",
    "\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b55fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = None  # Set to None to use all images\n",
    "batch_size = 2\n",
    "\n",
    "train_dataset = SolarDataset(\"new_dataset/training/low_res\", \n",
    "                             \"new_dataset/training/high_res\", \n",
    "                             augment=True, max_images=max_images)\n",
    "\n",
    "val_dataset = SolarDataset(\"new_dataset/validation/low_res\", \n",
    "                           \"new_dataset/validation/high_res\", \n",
    "                           augment=False, max_images=max_images)\n",
    "\n",
    "test_dataset = SolarDataset(\"new_dataset/testing/low_res\", \n",
    "                            \"new_dataset/testing/high_res\", \n",
    "                            augment=False, max_images=max_images)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1359275a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7229335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# -----------------------------------\n",
    "# DEVICE SETUP\n",
    "# -----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "# -----------------------------------\n",
    "# LOSS FUNCTION\n",
    "# -----------------------------------\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# -----------------------------------\n",
    "# MODEL & OPTIMIZER\n",
    "# -----------------------------------\n",
    " # custom file or your model class\n",
    "\n",
    "model = SwinIR(\n",
    "    upscale=4,\n",
    "    in_chans=3,\n",
    "    img_size=64,\n",
    "    window_size=8,\n",
    "    img_range=1.0,\n",
    "    depths=[6,6,6,6],\n",
    "    embed_dim=60,\n",
    "    num_heads=[6,6,6,6],\n",
    "    mlp_ratio=2,\n",
    "    upsampler='pixelshuffle'\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# -----------------------------------\n",
    "# CHECKPOINT DIRECTORY\n",
    "# -----------------------------------\n",
    "checkpoint_dir = \"checkpoints_swin_l1\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_val_ssim = -1.0\n",
    "epochs = 50\n",
    "\n",
    "# -----------------------------------\n",
    "# HELPER METRIC FUNCTIONS\n",
    "# -----------------------------------\n",
    "def psnr(pred, target):\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "\n",
    "def pixelwise_error(pred, target):\n",
    "    return torch.mean(torch.abs(pred - target))\n",
    "\n",
    "# -----------------------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------------------\n",
    "scaler = torch.cuda.amp.GradScaler()  # mixed precision for speed\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for lr_imgs, hr_imgs in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # mixed precision\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * lr_imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # VALIDATION\n",
    "    # -----------------------------------\n",
    "    model.eval()\n",
    "    val_loss, pixel_err, psnr_val, ssim_val = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "            outputs = model(lr_imgs)\n",
    "\n",
    "            val_loss += criterion(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "            pixel_err += pixelwise_error(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "            psnr_val += psnr(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "            ssim_val += ssim(outputs, hr_imgs, data_range=1.0, size_average=True).item() * lr_imgs.size(0)\n",
    "\n",
    "    n_val = len(val_loader.dataset)\n",
    "    val_loss /= n_val\n",
    "    pixel_err /= n_val\n",
    "    psnr_val /= n_val\n",
    "    ssim_val /= n_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.6f} | \"\n",
    "          f\"Val Loss: {val_loss:.6f} | \"\n",
    "          f\"Pixel Err: {pixel_err:.6f} | \"\n",
    "          f\"PSNR: {psnr_val:.2f} dB | \"\n",
    "          f\"SSIM: {ssim_val:.4f}\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # CHECKPOINT SAVING\n",
    "    # -----------------------------------\n",
    "    if ssim_val > best_val_ssim:\n",
    "        best_val_ssim = ssim_val\n",
    "        ckpt_path = os.path.join(checkpoint_dir, \"best_swin_sr.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_ssim': best_val_ssim\n",
    "        }, ckpt_path)\n",
    "        print(f\"✅ Best model updated (SSIM={ssim_val:.4f}) at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/104 [00:00<?, ?it/s]c:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "Testing:   0%|          | 0/104 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Predictions and targets are expected to have the same shape, but got torch.Size([1, 8, 1, 256, 256]) and torch.Size([1, 1, 256, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     psnr_list\u001b[38;5;241m.\u001b[39mappend(psnr(pred_img, true_img))\n\u001b[1;32m---> 57\u001b[0m     ssim_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mssim_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_img_ssim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_img_ssim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     58\u001b[0m     pixel_err_list\u001b[38;5;241m.\u001b[39mappend(pixelwise_error(pred_img, true_img))\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Save reconstructed image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36mssim_metric\u001b[1;34m(pred, target)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mssim_metric\u001b[39m(pred, target):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\torchmetrics\\functional\\image\\_deprecated.py:214\u001b[0m, in \u001b[0;36m_structural_similarity_index_measure\u001b[1;34m(preds, target, gaussian_kernel, sigma, kernel_size, reduction, data_range, k1, k2, return_full_image, return_contrast_sensitivity)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for deprecated import.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m>>> import torch\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m _deprecated_root_import_func(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral_angle_mapper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstructural_similarity_index_measure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgaussian_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgaussian_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_full_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_contrast_sensitivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_contrast_sensitivity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\torchmetrics\\functional\\image\\ssim.py:273\u001b[0m, in \u001b[0;36mstructural_similarity_index_measure\u001b[1;34m(preds, target, gaussian_kernel, sigma, kernel_size, reduction, data_range, k1, k2, return_full_image, return_contrast_sensitivity)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstructural_similarity_index_measure\u001b[39m(\n\u001b[0;32m    211\u001b[0m     preds: Tensor,\n\u001b[0;32m    212\u001b[0m     target: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m     return_contrast_sensitivity: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    222\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor]]:\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Structural Similarity Index Measure.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     preds, target \u001b[38;5;241m=\u001b[39m \u001b[43m_ssim_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     similarity_pack \u001b[38;5;241m=\u001b[39m _ssim_update(\n\u001b[0;32m    275\u001b[0m         preds,\n\u001b[0;32m    276\u001b[0m         target,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m         return_contrast_sensitivity,\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(similarity_pack, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\torchmetrics\\functional\\image\\ssim.py:37\u001b[0m, in \u001b[0;36m_ssim_check_inputs\u001b[1;34m(preds, target)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m     36\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(preds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m---> 37\u001b[0m \u001b[43m_check_same_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `preds` and `target` to have BxCxHxW or BxCxDxHxW shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got preds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and target: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:39\u001b[0m, in \u001b[0;36m_check_same_shape\u001b[1;34m(preds, target)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that predictions and target have the same shape, else raise error.\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and targets are expected to have the same shape, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Predictions and targets are expected to have the same shape, but got torch.Size([1, 8, 1, 256, 256]) and torch.Size([1, 1, 256, 256])."
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def pixelwise_error(pred, target):\n",
    "    return torch.mean(torch.abs(pred - target)).item()\n",
    "\n",
    "def psnr(pred, target, max_val=1.0):\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * torch.log10(max_val / torch.sqrt(mse)).item()\n",
    "\n",
    "def ssim_metric(pred, target):\n",
    "    return ssim(pred, target, data_range=1.0).item()\n",
    "\n",
    "# -----------------------------\n",
    "# Load model\n",
    "# -----------------------------\n",
    "model = SwinSRNet(sw_model_name=\"swin_tiny_patch4_window7_224\", pretrained=False, upscale=4).to(device)\n",
    "model.load_state_dict(torch.load(\"checkpoints_swin_l1/best_swin_sr.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(\"reconstructed_images_swin\", exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Testing and Visualization\n",
    "# -----------------------------\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "pixel_err_list = []\n",
    "\n",
    "count = 0\n",
    "for lr_imgs, hr_imgs in tqdm(test_loader, desc=\"Testing\"):\n",
    "    lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(lr_imgs)\n",
    "\n",
    "    for i in range(lr_imgs.size(0)):\n",
    "        pred_img = outputs[i].detach().cpu().float()\n",
    "        true_img = hr_imgs[i].detach().cpu().float()\n",
    "\n",
    "        pred_img_ssim = pred_img.unsqueeze(0)\n",
    "        true_img_ssim = true_img.unsqueeze(0)\n",
    "\n",
    "        psnr_list.append(psnr(pred_img, true_img))\n",
    "        ssim_list.append(ssim_metric(pred_img_ssim, true_img_ssim))\n",
    "        pixel_err_list.append(pixelwise_error(pred_img, true_img))\n",
    "\n",
    "        recon_img_path = f\"reconstructed_images_swin/recon_{count}.png\"\n",
    "        plt.imsave(recon_img_path, pred_img.squeeze(), cmap='gray')\n",
    "\n",
    "        if count < 5:\n",
    "            fig, axes = plt.subplots(1,2, figsize=(6,3))\n",
    "            axes[0].imshow(true_img.squeeze(), cmap='gray')\n",
    "            axes[0].set_title(\"Original\")\n",
    "            axes[0].axis('off')\n",
    "            axes[1].imshow(pred_img.squeeze(), cmap='gray')\n",
    "            axes[1].set_title(\"Reconstructed\")\n",
    "            axes[1].axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "print(f\"Average PSNR: {np.mean(psnr_list):.2f} dB\")\n",
    "print(f\"Average SSIM: {np.mean(ssim_list):.4f}\")\n",
    "print(f\"Average Pixel Error: {np.mean(pixel_err_list):.6f}\")\n",
    "print(\"Reconstructed images saved in 'reconstructed_images_swin/' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "c:\\Users\\bhatt\\miniconda3\\envs\\torch118\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SwinIR from swinir_model.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 0/776 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# train_swin_solar_sr.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pytorch_msssim import ssim as ms_ssim\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility / device\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# Augmentations (your functions)\n",
    "# ---------------------------\n",
    "def radial_vignette(img, strength=0.2):\n",
    "    h, w = img.shape\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    cy, cx = h / 2, w / 2\n",
    "    r = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "    r = r / r.max()\n",
    "    mask = 1 - strength * (r**2)\n",
    "    vignette = img.astype(np.float32) * mask\n",
    "    vignette = np.clip(vignette, 0, 255)\n",
    "    return vignette.astype(np.uint8)\n",
    "\n",
    "def add_poisson_noise(img, scale_low=5.0, scale_high=100.0):\n",
    "    if img.dtype != np.float32:\n",
    "        img_f = img.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        img_f = img\n",
    "    scale = random.uniform(scale_low, scale_high)\n",
    "    noisy = np.random.poisson(img_f * scale) / float(scale)\n",
    "    noisy = np.clip(noisy, 0.0, 1.0)\n",
    "    return (noisy * 255).astype(np.uint8)\n",
    "\n",
    "def random_rotation(img, angle_range=20):\n",
    "    h, w = img.shape\n",
    "    angle = random.uniform(-angle_range, angle_range)\n",
    "    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "    rotated = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    return rotated\n",
    "\n",
    "def random_crop_resize(img, crop_scale=0.8, out_size=(256, 256)):\n",
    "    h, w = img.shape\n",
    "    ch, cw = int(h * crop_scale), int(w * crop_scale)\n",
    "    if ch == 0 or cw == 0:\n",
    "        return cv2.resize(img, out_size, interpolation=cv2.INTER_LINEAR)\n",
    "    y = random.randint(0, h - ch)\n",
    "    x = random.randint(0, w - cw)\n",
    "    crop = img[y:y+ch, x:x+cw]\n",
    "    resized = cv2.resize(crop, out_size, interpolation=cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "def gaussian_blur(img, sigma_range=(0.5, 2.0)):\n",
    "    sigma = random.uniform(*sigma_range)\n",
    "    ksize = int(2 * round(3*sigma) + 1)\n",
    "    if ksize % 2 == 0:\n",
    "        ksize += 1\n",
    "    blurred = cv2.GaussianBlur(img, (ksize, ksize), sigmaX=sigma)\n",
    "    return blurred\n",
    "\n",
    "def brightness_contrast_jitter(img, brightness=0.2, contrast=0.3):\n",
    "    b = random.uniform(-brightness, brightness) * 255\n",
    "    c = 1.0 + random.uniform(-contrast, contrast)\n",
    "    jittered = img.astype(np.float32) * c + b\n",
    "    jittered = np.clip(jittered, 0, 255)\n",
    "    return jittered.astype(np.uint8)\n",
    "\n",
    "def random_noise(img, noise_level=0.10):\n",
    "    if img.dtype != np.float32:\n",
    "        img_f = img.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        img_f = img\n",
    "    noise = np.random.normal(0, noise_level, img_f.shape).astype(np.float32)\n",
    "    noisy = img_f + noise\n",
    "    noisy = np.clip(noisy, 0.0, 1.0)\n",
    "    return (noisy * 255).astype(np.uint8)\n",
    "\n",
    "# ---------------------------\n",
    "# Model: try official SwinIR else fallback timm-based simple head\n",
    "# ---------------------------\n",
    "try:\n",
    "    # If network_swinir.py is present and defines SwinIR\n",
    "    from swinir_model import SwinIR    # user-provided network_swinir.py saved as swinir_model.py\n",
    "    print(\"Using SwinIR from swinir_model.py\")\n",
    "    def build_model(in_chans=1, upscale=4):\n",
    "        # instantiate a typical SwinIR-like config for medium model (you can change)\n",
    "        return SwinIR(\n",
    "            upscale=upscale,\n",
    "            in_chans=in_chans,\n",
    "            img_size=64,\n",
    "            window_size=8,\n",
    "            img_range=1.0,\n",
    "            depths=[6,6,6,6],\n",
    "            embed_dim=60,\n",
    "            num_heads=[6,6,6,6],\n",
    "            mlp_ratio=2,\n",
    "            upsampler='pixelshuffle'\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(\"Official SwinIR import failed or not found. Falling back to a timm-based Swin-T backbone + simple reconstruction head.\")\n",
    "    print(\"Import error was:\", e)\n",
    "    # fallback model using timm\n",
    "    try:\n",
    "        from timm import create_model\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"timm not installed. Install it with `pip install timm` or provide network_swinir.py in the working directory.\")\n",
    "    class SwinFallbackSR(nn.Module):\n",
    "        def __init__(self, sw_model_name=\"swin_tiny_patch4_window7_224\", pretrained=True, in_chans=1, upscale=4):\n",
    "            super().__init__()\n",
    "            self.in_chans = in_chans\n",
    "            self.upscale = upscale\n",
    "            # create backbone that returns intermediate feature maps\n",
    "            self.backbone = create_model(sw_model_name, pretrained=pretrained, features_only=True, in_chans=in_chans)\n",
    "            # get channels of the last feature map\n",
    "            try:\n",
    "                backbone_out = self.backbone.feature_info[-1]['num_chs']\n",
    "            except Exception:\n",
    "                # fallback guess\n",
    "                backbone_out = 768\n",
    "            # reconstruction: upsample then conv to output channels\n",
    "            self.reconstruct = nn.Sequential(\n",
    "                nn.Conv2d(backbone_out, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Upsample(scale_factor=upscale, mode='bilinear', align_corners=False),\n",
    "                nn.Conv2d(256, in_chans, kernel_size=3, padding=1),\n",
    "                nn.Sigmoid()   # ensure output in [0,1]\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            feats = self.backbone(x)[-1]    # last feature map\n",
    "            out = self.reconstruct(feats)\n",
    "            return out\n",
    "    def build_model(in_chans=1, upscale=4):\n",
    "        return SwinFallbackSR(in_chans=in_chans, upscale=upscale)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset\n",
    "# ---------------------------\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, augment=True, img_size=(256,256), max_images=None):\n",
    "        self.lr_files = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tif'))])\n",
    "        self.hr_files = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tif'))])\n",
    "        if max_images is not None:\n",
    "            self.lr_files = self.lr_files[:max_images]\n",
    "            self.hr_files = self.hr_files[:max_images]\n",
    "        assert len(self.lr_files) == len(self.hr_files), \"LR and HR file counts differ!\"\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = cv2.imread(self.lr_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        hr = cv2.imread(self.hr_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if lr is None or hr is None:\n",
    "            raise RuntimeError(f\"Failed to load image pair: {self.lr_files[idx]}, {self.hr_files[idx]}\")\n",
    "        lr = cv2.resize(lr, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "        hr = cv2.resize(hr, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.augment:\n",
    "            # random order of augmentation\n",
    "            if random.random() < 0.9:\n",
    "                lr = radial_vignette(lr, strength=random.uniform(0.03,0.18))\n",
    "            if random.random() < 0.8:\n",
    "                lr = add_poisson_noise(lr)\n",
    "            if random.random() < 0.6:\n",
    "                lr = random_rotation(lr, angle_range=12)\n",
    "            if random.random() < 0.7:\n",
    "                lr = gaussian_blur(lr, sigma_range=(0.3,1.2))\n",
    "            if random.random() < 0.7:\n",
    "                lr = brightness_contrast_jitter(lr, brightness=0.12, contrast=0.18)\n",
    "            if random.random() < 0.5:\n",
    "                lr = random_noise(lr, noise_level=0.04)\n",
    "\n",
    "        # convert to tensor CxHxW in float32 [0,1]\n",
    "        lr_t = torch.tensor(lr, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "        hr_t = torch.tensor(hr, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "        return lr_t, hr_t\n",
    "\n",
    "# ---------------------------\n",
    "# Config and DataLoaders\n",
    "# ---------------------------\n",
    "max_images = None\n",
    "batch_size = 8\n",
    "img_size = (256,256)\n",
    "upscale = 4\n",
    "\n",
    "train_dataset = SolarDataset(\"new_dataset/training/low_res\",\n",
    "                             \"new_dataset/training/high_res\",\n",
    "                             augment=True, img_size=img_size, max_images=max_images)\n",
    "\n",
    "val_dataset = SolarDataset(\"new_dataset/validation/low_res\",\n",
    "                           \"new_dataset/validation/high_res\",\n",
    "                           augment=False, img_size=img_size, max_images=max_images)\n",
    "\n",
    "test_dataset = SolarDataset(\"new_dataset/testing/low_res\",\n",
    "                            \"new_dataset/testing/high_res\",\n",
    "                            augment=False, img_size=img_size, max_images=max_images)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Build model, loss, optimizer\n",
    "# ---------------------------\n",
    "in_chans = 1\n",
    "try:\n",
    "    model = build_model(in_chans=in_chans, upscale=upscale)\n",
    "except TypeError:\n",
    "    # fallback if build_model signature uses different args\n",
    "    model = build_model(in_chans, upscale)\n",
    "# ensure final activation range match training targets: many implementations expect output in [0,1]\n",
    "# If using official SwinIR that outputs raw range, we'll clamp later.\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def psnr_torch(pred, target):\n",
    "    # pred, target in [0,1]\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse.item() == 0:\n",
    "        return torch.tensor(100.0, device=pred.device)\n",
    "    return 20.0 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "\n",
    "def pixelwise_error(pred, target):\n",
    "    return torch.mean(torch.abs(pred - target))\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop (no mixed precision)\n",
    "# ---------------------------\n",
    "checkpoint_dir = \"checkpoints_swin_l1\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_val_ssim = -1.0\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    for lr_imgs, hr_imgs in pbar:\n",
    "        lr_imgs = lr_imgs.to(device)   # float32\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(lr_imgs)\n",
    "        # If model returns outside [0,1], ensure we map to [0,1] for loss with HR in [0,1]\n",
    "        # If using official SwinIR with img_range=1.0 it should already be in [0,1]; else clamp:\n",
    "        outputs = torch.clamp(outputs, 0.0, 1.0)\n",
    "\n",
    "        loss = criterion(outputs, hr_imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * lr_imgs.size(0)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    pixel_err = 0.0\n",
    "    psnr_val = 0.0\n",
    "    ssim_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "            outputs = model(lr_imgs)\n",
    "            outputs = torch.clamp(outputs, 0.0, 1.0)\n",
    "\n",
    "            val_loss += criterion(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "            pixel_err += pixelwise_error(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "            psnr_val += psnr_torch(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "            # ms_ssim expects images in [0,1] and shape N,C,H,W\n",
    "            ssim_val += ms_ssim(outputs, hr_imgs, data_range=1.0, size_average=True).item() * lr_imgs.size(0)\n",
    "\n",
    "    n_val = len(val_loader.dataset)\n",
    "    val_loss /= n_val\n",
    "    pixel_err /= n_val\n",
    "    psnr_val /= n_val\n",
    "    ssim_val /= n_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Pixel Err: {pixel_err:.6f} | PSNR: {psnr_val:.2f} dB | SSIM: {ssim_val:.4f}\")\n",
    "\n",
    "    # Save best + latest checkpoints\n",
    "    ckpt_latest = os.path.join(checkpoint_dir, \"latest_swin_sr.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'ssim': ssim_val\n",
    "    }, ckpt_latest)\n",
    "\n",
    "    if ssim_val > best_val_ssim:\n",
    "        best_val_ssim = ssim_val\n",
    "        ckpt_best = os.path.join(checkpoint_dir, \"best_swin_sr.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'ssim': ssim_val\n",
    "        }, ckpt_best)\n",
    "        print(f\"✅ Best model updated (SSIM={ssim_val:.4f}) at epoch {epoch+1}\")\n",
    "\n",
    "# Optionally: run test inference using test_loader and save outputs\n",
    "model.eval()\n",
    "out_dir = \"sr_results\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i, (lr_imgs, hr_imgs) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        outputs = torch.clamp(model(lr_imgs), 0.0, 1.0)\n",
    "        # save first example in batch\n",
    "        out_np = (outputs[0,0].cpu().numpy() * 255.0).astype(np.uint8)\n",
    "        cv2.imwrite(os.path.join(out_dir, f\"sr_{i:04d}.png\"), out_np)\n",
    "print(\"Training finished. Best SSIM:\", best_val_ssim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
