{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T06:21:31.041550Z",
     "iopub.status.busy": "2025-10-17T06:21:31.040971Z",
     "iopub.status.idle": "2025-10-17T06:21:31.047656Z",
     "shell.execute_reply": "2025-10-17T06:21:31.047040Z",
     "shell.execute_reply.started": "2025-10-17T06:21:31.041525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-17T06:21:31.049009Z",
     "iopub.status.busy": "2025-10-17T06:21:31.048773Z",
     "iopub.status.idle": "2025-10-17T06:21:32.634326Z",
     "shell.execute_reply": "2025-10-17T06:21:32.633491Z",
     "shell.execute_reply.started": "2025-10-17T06:21:31.048994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### MODEL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RLFB(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.act1 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.act2 = nn.SiLU()\n",
    "        self.conv3 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.act3 = nn.SiLU()\n",
    "        # 1x1 conv to reduce/restore channels and connect to ESA per paper figure\n",
    "        self.conv_reduce = nn.Conv2d(channels, channels, kernel_size=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x); out = self.act1(out)\n",
    "        out = self.conv2(out); out = self.act2(out)\n",
    "        out = self.conv3(out); out = self.act3(out)\n",
    "        out = self.conv_reduce(out)\n",
    "        return x + out\n",
    "\n",
    "class ESA(nn.Module):\n",
    "    def __init__(self, channels=64, reduction=4):\n",
    "        super().__init__()\n",
    "        mid = channels // reduction\n",
    "        self.conv1 = nn.Conv2d(channels, mid, kernel_size=1, padding=0)\n",
    "        # downsample block: large receptive pooling as paper: use maxpool(7, stride=3) as described\n",
    "        self.pool = nn.MaxPool2d(kernel_size=7, stride=3, padding=3)  # padding to keep shapes reasonable\n",
    "        self.conv2 = nn.Conv2d(mid, mid, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(mid, mid, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(mid, channels, kernel_size=1, padding=0)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # small bilinear upsampling used in paper (interpolation)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,H,W)\n",
    "        c1 = self.conv1(x)          # reduce channels\n",
    "        p = self.pool(c1)          # spatial reduce\n",
    "        p = self.relu(self.conv2(p))\n",
    "        p = self.relu(self.conv3(p))\n",
    "        # upsample back to original spatial (use bilinear)\n",
    "        p_up = F.interpolate(p, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n",
    "        p_up = self.conv4(p_up)\n",
    "        att = self.sig(p_up)\n",
    "        return x * att\n",
    "\n",
    "class MESRGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, num_features=64, num_rlfb=12, scale=4):\n",
    "        \"\"\"\n",
    "        in_channels: typically 1 for magnetograms (paper uses single-channel)\n",
    "        out_channels: 1\n",
    "        num_features: 64 (paper uses 64)\n",
    "        num_rlfb: 12 stacked RLFB blocks as paper\n",
    "        scale: 4 (paper performs 4x super-resolution)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.shallow = nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1)\n",
    "        self.rlfb_blocks = nn.Sequential(*[RLFB(num_features) for _ in range(num_rlfb)])\n",
    "        self.esa = ESA(num_features)\n",
    "        self.fuse = nn.Conv2d(num_features, num_features, kernel_size=1, padding=0)\n",
    "        ups = []\n",
    "        s = scale\n",
    "        while s > 1:\n",
    "            ups += [nn.Conv2d(num_features, num_features * 4, kernel_size=3, padding=1),\n",
    "                    nn.PixelShuffle(2),\n",
    "                    nn.ReLU(inplace=True)]\n",
    "            s //= 2\n",
    "        self.upsampler = nn.Sequential(*ups)\n",
    "        # final reconstruction conv\n",
    "        self.recon = nn.Conv2d(num_features, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: LR image tensor (B, C, H, W) where H,W are LR spatial dims\n",
    "        feat_shallow = self.shallow(x)           # shallow features\n",
    "        feat = self.rlfb_blocks(feat_shallow)    # deep RLFB stack\n",
    "        feat = self.esa(feat)                    # ESA attention\n",
    "        feat = self.fuse(feat + feat_shallow)    # residual-style fusion (paper connects features)\n",
    "        out = self.upsampler(feat)               # upsample to HR\n",
    "        out = self.recon(out)\n",
    "        return out\n",
    "\n",
    "class SRGAN_D2_Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        # Following the conv/layer pattern in user's class with similar channel counts/strides\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 3, stride=1, padding=1)  # act LeakyReLU\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1, bias=True)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1, bias=True)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1, bias=True)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1, bias=True)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Flatten + dense head\n",
    "        # We'll use AdaptiveAvgPool2d to produce fixed-size before linear head to match original flatten behavior\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # output size 1x1 per channel\n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x expected (B, C, H, W)\n",
    "        x = self.lrelu(self.conv1(x))\n",
    "        x = self.lrelu(self.conv2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu(self.conv3(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu(self.conv4(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.lrelu(self.conv5(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.lrelu(self.conv6(x))\n",
    "        x = self.bn5(x)\n",
    "        x = self.lrelu(self.conv7(x))\n",
    "        x = self.bn6(x)\n",
    "        x = self.lrelu(self.conv8(x))\n",
    "        x = self.bn7(x)\n",
    "\n",
    "        x = self.pool(x)         # (B, 512, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 512)\n",
    "        x = self.lrelu(self.fc1(x))\n",
    "        logits = self.fc2(x)     # raw logits (B,1)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        return prob, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T06:21:32.636412Z",
     "iopub.status.busy": "2025-10-17T06:21:32.635829Z",
     "iopub.status.idle": "2025-10-17T06:21:34.028363Z",
     "shell.execute_reply": "2025-10-17T06:21:34.027769Z",
     "shell.execute_reply.started": "2025-10-17T06:21:32.636392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Pre-Processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_solar_image(img_path, apply_clahe=False, wavelet='db2', levels=2):\n",
    "    # ----------------------\n",
    "    # 1. Load & normalize\n",
    "    # ----------------------\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "    img /= img.max() + 1e-8\n",
    "\n",
    "    # ----------------------\n",
    "    # 2. Dynamic range control\n",
    "    # ----------------------\n",
    "    if apply_clahe:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img_eq = clahe.apply((img*255).astype(np.uint8)) / 255.0\n",
    "    else:\n",
    "        img_eq = np.log1p(img) / np.log(2.0)  # log scaling for high dynamic range\n",
    "\n",
    "    features = [img_eq]\n",
    "\n",
    "    # ----------------------\n",
    "    # 3. Multiscale decomposition (Laplacian pyramid)\n",
    "    # ----------------------\n",
    "    current = img_eq.copy()\n",
    "    laplacian_levels = []\n",
    "    for _ in range(levels):\n",
    "        down = cv2.pyrDown(current)\n",
    "        up = cv2.pyrUp(down, dstsize=current.shape[::-1])\n",
    "        lap = current - up\n",
    "        laplacian_levels.append(cv2.resize(lap, img_eq.shape[::-1]))\n",
    "        # laplacian_levels.append(lap)\n",
    "        current = down\n",
    "    features.extend(laplacian_levels)\n",
    "\n",
    "    # ----------------------\n",
    "    # 4. Frequency / edge filters\n",
    "    # ----------------------\n",
    "\n",
    "    # Laplacian high-pass\n",
    "    lap = cv2.Laplacian(img_eq, cv2.CV_32F, ksize=3)\n",
    "\n",
    "    # Sobel edges\n",
    "    sobelx = cv2.Sobel(img_eq, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(img_eq, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    sobel_mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "\n",
    "    # DoG (Difference of Gaussians)\n",
    "    g1 = cv2.GaussianBlur(img_eq, (3,3), 0.5)\n",
    "    g2 = cv2.GaussianBlur(img_eq, (3,3), 1.5)\n",
    "    dog = g1 - g2\n",
    "\n",
    "    # Optional: Gabor filter for orientation-sensitive structures\n",
    "    gabor_kernels = []\n",
    "    for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:\n",
    "        kern = cv2.getGaborKernel((7,7), 2.0, theta, 5.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "        gabor_resp = cv2.filter2D(img_eq, cv2.CV_32F, kern)\n",
    "        gabor_kernels.append(gabor_resp)\n",
    "\n",
    "    features.extend([lap, sobel_mag, dog] + gabor_kernels)\n",
    "\n",
    "    # ----------------------\n",
    "    # 5. Concatenate all features\n",
    "    # ----------------------\n",
    "    stacked = np.stack(features, axis=0)  # shape: [C, H, W]\n",
    "    tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "\n",
    "    return tensor  # ready for model input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T06:21:34.029345Z",
     "iopub.status.busy": "2025-10-17T06:21:34.029058Z",
     "iopub.status.idle": "2025-10-17T06:21:34.045196Z",
     "shell.execute_reply": "2025-10-17T06:21:34.044635Z",
     "shell.execute_reply.started": "2025-10-17T06:21:34.029328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### LOSS FUNCTION\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pytorch_msssim import ms_ssim\n",
    "\n",
    "def calc_psnr(pred, target, max_val=1.0):\n",
    "    mse = F.mse_loss(pred, target)\n",
    "    return 10 * torch.log10(max_val ** 2 / (mse + 1e-8))\n",
    "\n",
    "def tv_loss(img):\n",
    "    \"\"\"Total Variation loss\"\"\"\n",
    "    dh = torch.mean(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]))\n",
    "    dw = torch.mean(torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]))\n",
    "    return dh + dw\n",
    "\n",
    "class HaarDWT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # fixed filters\n",
    "        ll = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32) / 2\n",
    "        lh = torch.tensor([[1, 1], [-1, -1]], dtype=torch.float32) / 2\n",
    "        hl = torch.tensor([[1, -1], [1, -1]], dtype=torch.float32) / 2\n",
    "        hh = torch.tensor([[1, -1], [-1, 1]], dtype=torch.float32) / 2\n",
    "        self.register_buffer(\"filters\", torch.stack([ll, lh, hl, hh], dim=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        filt = self.filters.to(x.device).unsqueeze(1)  # (4,1,2,2)\n",
    "        filt = filt.repeat(1, C, 1, 1)                 # (4,C,2,2)\n",
    "        y = F.conv2d(x, filt, stride=2, padding=0, groups=C)  # (B, 4*C, H/2, W/2)\n",
    "        y = y.view(B, 4, C, H // 2, W // 2)\n",
    "        return y[:, 1], y[:, 2], y[:, 3]  # LH, HL, HH\n",
    "\n",
    "class VGGPerceptual(nn.Module):\n",
    "    def __init__(self, layer_index=16):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "        self.slice = nn.Sequential(*[vgg[i] for i in range(layer_index + 1)])\n",
    "        for p in self.slice.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.slice(x)\n",
    "\n",
    "\n",
    "\n",
    "class TotalLoss(nn.Module):\n",
    "    def __init__(self, device, alpha=1.0, beta=0.5, gamma=0.2, delta=0.01, eps_tv=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.eps_tv = eps_tv\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.vgg = VGGPerceptual(layer_index=16).to(device)\n",
    "        self.wavelet = HaarDWT().to(device)\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        # L1\n",
    "        l1_loss = self.l1(sr, hr)\n",
    "\n",
    "        # MS-SSIM (use [0,1] range)\n",
    "        ms_loss = 1 - ms_ssim(sr, hr, data_range=1.0, size_average=True)\n",
    "\n",
    "        # Wavelet L2 on high-frequency bands\n",
    "        sr_LH, sr_HL, sr_HH = self.wavelet(sr)\n",
    "        hr_LH, hr_HL, hr_HH = self.wavelet(hr)\n",
    "        wave_l2 = F.mse_loss(sr_LH, hr_LH) + F.mse_loss(sr_HL, hr_HL) + F.mse_loss(sr_HH, hr_HH)\n",
    "\n",
    "        # Perceptual L2 (VGG features)\n",
    "        if sr.shape[1] == 1:\n",
    "            sr = sr.repeat(1, 3, 1, 1)\n",
    "            hr = hr.repeat(1, 3, 1, 1)\n",
    "        feat_sr = self.vgg(sr)\n",
    "        feat_hr = self.vgg(hr)\n",
    "        perceptual = F.mse_loss(feat_sr, feat_hr)\n",
    "\n",
    "        # TV\n",
    "        tv = tv_loss(sr)\n",
    "\n",
    "        total = (self.alpha * l1_loss +\n",
    "                 self.beta * ms_loss +\n",
    "                 self.gamma * wave_l2 +\n",
    "                 self.delta * perceptual +\n",
    "                 self.eps_tv * tv)\n",
    "        return total, {\n",
    "            \"L1\": l1_loss.item(),\n",
    "            \"MS-SSIM\": ms_loss.item(),\n",
    "            \"Wavelet\": wave_l2.item(),\n",
    "            \"Perceptual\": perceptual.item(),\n",
    "            \"TV\": tv.item(),\n",
    "            \"Total\": total.item(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T06:21:34.046878Z",
     "iopub.status.busy": "2025-10-17T06:21:34.046674Z",
     "iopub.status.idle": "2025-10-17T06:21:34.063047Z",
     "shell.execute_reply": "2025-10-17T06:21:34.062373Z",
     "shell.execute_reply.started": "2025-10-17T06:21:34.046863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SolarSRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, apply_clahe=False):\n",
    "        self.lr_files = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.tif'))])\n",
    "        self.hr_files = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.tif'))])\n",
    "        assert len(self.lr_files) == len(self.hr_files), \"LR and HR folders must have same number of images\"\n",
    "        self.apply_clahe = apply_clahe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_tensor = preprocess_solar_image(self.lr_files[idx], apply_clahe=self.apply_clahe)  # (10,H,W)\n",
    "        hr_tensor = preprocess_solar_image(self.hr_files[idx], apply_clahe=self.apply_clahe)  # (10,H,W)\n",
    "        # the HR target is grayscale single-channel — take only the equalized version\n",
    "        hr_tensor = hr_tensor[0:1, :, :]   # keep only first channel as GT\n",
    "        return lr_tensor, hr_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T06:21:34.063797Z",
     "iopub.status.busy": "2025-10-17T06:21:34.063610Z",
     "iopub.status.idle": "2025-10-17T06:21:34.133095Z",
     "shell.execute_reply": "2025-10-17T06:21:34.132220Z",
     "shell.execute_reply.started": "2025-10-17T06:21:34.063780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_sr(\n",
    "    train_lr_dir, train_hr_dir,\n",
    "    valid_lr_dir, valid_hr_dir,\n",
    "    epochs=50, batch_size=4,\n",
    "    lr_gen=1e-4, lr_disc=1e-4,\n",
    "    adv_weight=1e-3,\n",
    "    save_dir=\"checkpoints\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # --- Create datasets & loaders ---\n",
    "    train_ds = SolarSRDataset(train_lr_dir, train_hr_dir)\n",
    "    val_ds = SolarSRDataset(valid_lr_dir, valid_hr_dir)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    # --- Model setup ---\n",
    "    in_ch = 10  # from your preprocessing (10 stacked feature maps)\n",
    "    gen = MESRGenerator(in_channels=in_ch, out_channels=1, num_features=64, num_rlfb=12, scale=4).to(device)\n",
    "    disc = SRGAN_D2_Discriminator(in_channels=1).to(device)\n",
    "\n",
    "    opt_g = torch.optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.9, 0.999))\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=lr_disc, betas=(0.9, 0.999))\n",
    "\n",
    "    total_loss_fn = TotalLoss(device).to(device)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    # =========================\n",
    "    #      Training Loop\n",
    "    # =========================\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        gen.train(); disc.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch [{epoch}/{epochs}]\")\n",
    "        avg_g, avg_d = 0, 0\n",
    "\n",
    "        for lr_imgs, hr_imgs in loop:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "\n",
    "            # --- Train Discriminator ---\n",
    "            opt_d.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                sr_fake = gen(lr_imgs)\n",
    "            real_prob, real_logit = disc(hr_imgs)\n",
    "            fake_prob, fake_logit = disc(sr_fake.detach())\n",
    "            real_lbl = torch.ones_like(real_logit)\n",
    "            fake_lbl = torch.zeros_like(fake_logit)\n",
    "            d_loss = (bce_loss(real_logit, real_lbl) + bce_loss(fake_logit, fake_lbl)) * 0.5\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "\n",
    "            # --- Train Generator ---\n",
    "            opt_g.zero_grad()\n",
    "            sr = gen(lr_imgs)\n",
    "            fake_prob, fake_logit = disc(sr)\n",
    "            adv_loss = bce_loss(fake_logit, torch.ones_like(fake_logit))\n",
    "            total_loss, comps = total_loss_fn(sr, hr_imgs)\n",
    "            g_loss = total_loss + adv_weight * adv_loss\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "            avg_g += g_loss.item()\n",
    "            avg_d += d_loss.item()\n",
    "            loop.set_postfix({\n",
    "                \"G_total\": f\"{g_loss.item():.4f}\",\n",
    "                \"D_loss\": f\"{d_loss.item():.4f}\",\n",
    "                \"L1\": f\"{comps['L1']:.4f}\",\n",
    "                \"MS-SSIM\": f\"{comps['MS-SSIM']:.4f}\"\n",
    "            })\n",
    "\n",
    "        # --- Validation ---\n",
    "        gen.eval()\n",
    "        psnr_total, ssim_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr_imgs, hr_imgs in val_loader:\n",
    "                lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "                sr = gen(lr_imgs)\n",
    "                psnr_total += calc_psnr(sr, hr_imgs).item()\n",
    "                ssim_total += ms_ssim(sr, hr_imgs, data_range=1.0, size_average=True).item()\n",
    "\n",
    "        psnr_mean = psnr_total / len(val_loader)\n",
    "        ssim_mean = ssim_total / len(val_loader)\n",
    "        print(f\"Epoch {epoch}: PSNR={psnr_mean:.3f} dB | SSIM={ssim_mean:.4f}\")\n",
    "\n",
    "        if psnr_mean > best_psnr:\n",
    "            best_psnr = psnr_mean\n",
    "            torch.save(gen.state_dict(), os.path.join(save_dir, \"best_generator.pth\"))\n",
    "            torch.save(disc.state_dict(), os.path.join(save_dir, \"best_discriminator.pth\"))\n",
    "            print(f\"✅ Best model saved (PSNR={psnr_mean:.2f})\")\n",
    "\n",
    "    print(\"Training finished ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T06:21:34.134221Z",
     "iopub.status.busy": "2025-10-17T06:21:34.133927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]:  12%|█▏        | 93/776 [03:58<30:03,  2.64s/it, G_total=0.2980, D_loss=0.0876, L1=0.0771, MS-SSIM=0.2437] "
     ]
    }
   ],
   "source": [
    "train_sr(\n",
    "        train_lr_dir=\"/kaggle/input/solaresss/new_dataset/training/low_res\",\n",
    "        train_hr_dir=\"/kaggle/input/solaresss/new_dataset/training/high_res\",\n",
    "        valid_lr_dir=\"/kaggle/input/solaresss/new_dataset/validation/low_res\",\n",
    "        valid_hr_dir=\"/kaggle/input/solaresss/new_dataset/validation/high_res\",\n",
    "        epochs=20,\n",
    "        batch_size=8,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8484678,
     "sourceId": 13373578,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
